{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "- 대화 전체를 메모리에 담는다\n",
    "- 대화 전체를 담기 때문에 대화가 길어지면 메모리 사용랴잉 많아지고 비용이 증가한다\n",
    "- 자동완성 기능에 유용하다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 모든 대화를 저장한다.\n",
    "memory.save_context({\"input\": \"Hi!\"},{\"output\":\"How are you?\"})\n",
    "memory.save_context({\"input\": \"Hi!\"},{\"output\":\"How are you?\"})\n",
    "memory.save_context({\"input\": \"Hi!\"},{\"output\":\"How are you?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "\n",
    "- 대화의 특정 부분만 저장하는 메모리 ex: 최근 5개의 대화 저장\n",
    "- 메모리를 특정 사이즈로 유지하는데 장점\n",
    "- 단점은 최근 대화만 기억할 수 있다는 점\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k는 몇개의 대화를 저장할지에 대한 파라미터\n",
    "memory = ConversationBufferWindowMemory(\n",
    "  return_messages=True,\n",
    "  k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "add_message(5,5)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory\n",
    "\n",
    "- llm을 사용함\n",
    "- 메모리에 그냥 저장하는 것이 아니라 자체적으로 요약해서 저장함: 대화가 길어지면 토큰 비용을 절약할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=chat)\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm LMH, I live in South Korea\",\"Wow that is so cool!\")\n",
    "add_message(\"South Korea is so pretty!\",\"I wish I could go!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "\n",
    "- Summary Memory와 Buffer Memory의 결합\n",
    "- 메모리가 저장되는 개수를 추적하고 토큰 한계에 다다르면 대화를 요약해서 저장함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=chat, \n",
    "  max_token_limit=50, \n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "  return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm LMH, I live in South Korea\",\"Wow that is so cool!\")\n",
    "add_message(\"South Korea is so pretty!\",\"I wish I could go!!\")\n",
    "add_message(\"South Korean food is so delicious\",\"I wish I could eat them!!\")\n",
    "add_message(\"How far from Argentina?\",\"I don't know! Super Far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationKGMemory\n",
    "\n",
    "- KG = Knowledge Graph\n",
    "- 대화중의 엔티티의 KG를 생성함 : 가장 중요한 것만 뽑아내는 요약본 같은 것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationKGMemory(\n",
    "  llm=chat, \n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "  memory.save_context({\"input\":input},{\"output\":output})\n",
    "\n",
    "add_message(\"Hi I'm LMH, I live in South Korea\",\"Wow that is so cool!\")\n",
    "add_message(\"South Korea is so pretty!\",\"I wish I could go!!\")\n",
    "add_message(\"South Korean food is so delicious\",\"I wish I could eat them!!\")\n",
    "memory.load_memory_variables({\"input\" : \"Tell me about South Korea\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Chain\n",
    "\n",
    "- off-the-shelf : 일반적인 목적을 가진 chain\n",
    "- 이미 만들어진 chain으로 빠르게 시작할 때 좋지만 실제로는 커스텀해서 chain을 직접 만드는 경우가 더 많을 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=chat, \n",
    "  max_token_limit=100, \n",
    "  # return_messages=True,\n",
    "  memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "  You are a helpful AI talking to a human\n",
    "\n",
    "  {chat_history}\n",
    "  Human:{question}\n",
    "  You:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm = chat,\n",
    "  memory = memory,\n",
    "  prompt=PromptTemplate.from_template(template),\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplate 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=chat, \n",
    "  max_token_limit=100, \n",
    "  return_messages=True,\n",
    "  memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm = chat,\n",
    "  memory = memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")\n",
    "chain.predict(question=\"I live in Seoul\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL Based Memory Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, Nico! How can I assist you today?'\n",
      "content='Got it, Nico! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=chat, \n",
    "  max_token_limit=100, \n",
    "  return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"history\"),\n",
    "  (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | chat\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke({\n",
    "    \"question\":\"My name is Nico\"\n",
    "  })\n",
    "  memory.save_context({\"input\":question},{\"output\":result.content})\n",
    "  print(result)\n",
    "\n",
    "invoke_chain(\"My name is nico\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
